{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dea6e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text-v2-moe:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e30747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.024702996,\n",
       " 0.0034584615,\n",
       " -0.040929064,\n",
       " -0.034687277,\n",
       " 0.0010347337,\n",
       " -0.03119385,\n",
       " -0.011116706,\n",
       " 0.018843086,\n",
       " 0.064579956,\n",
       " 0.07633928,\n",
       " -0.009612688,\n",
       " -0.023541162,\n",
       " -0.02562753,\n",
       " 0.027421912,\n",
       " -0.019108627,\n",
       " 0.07003715,\n",
       " -0.049106266,\n",
       " -0.0144528495,\n",
       " 0.08121271,\n",
       " 0.020461991,\n",
       " 0.019015627,\n",
       " -0.03552293,\n",
       " 0.023020003,\n",
       " -0.04197235,\n",
       " -0.0102635,\n",
       " 0.0026943218,\n",
       " 0.028917521,\n",
       " -0.043186292,\n",
       " 0.09030084,\n",
       " 0.056978248,\n",
       " 0.011853291,\n",
       " -0.009509716,\n",
       " 0.03763759,\n",
       " 0.14317423,\n",
       " -0.0101060085,\n",
       " 0.027704835,\n",
       " 0.00695976,\n",
       " -0.04579651,\n",
       " 0.017377585,\n",
       " 0.020082923,\n",
       " 0.0373328,\n",
       " 0.008015837,\n",
       " -0.026376745,\n",
       " 0.03627828,\n",
       " 0.032916605,\n",
       " 0.029079448,\n",
       " 0.0013431854,\n",
       " -0.005824664,\n",
       " -0.047983572,\n",
       " -0.028385513,\n",
       " 0.020394653,\n",
       " -0.03852033,\n",
       " 0.04010152,\n",
       " 0.008131174,\n",
       " 0.016362935,\n",
       " -0.08599391,\n",
       " 0.0073413644,\n",
       " 0.043784127,\n",
       " 0.020328397,\n",
       " 0.018779913,\n",
       " 0.037062053,\n",
       " -0.058676332,\n",
       " 0.038892005,\n",
       " -0.022553701,\n",
       " 0.06904865,\n",
       " 0.0436354,\n",
       " -0.009012787,\n",
       " -0.008125674,\n",
       " 0.060701277,\n",
       " 0.032155186,\n",
       " -0.054253798,\n",
       " -0.06235148,\n",
       " 0.0670319,\n",
       " 0.030082075,\n",
       " -0.052139368,\n",
       " -0.0050154445,\n",
       " 0.060930323,\n",
       " 0.03770179,\n",
       " -0.03892548,\n",
       " 0.05655371,\n",
       " 0.012876323,\n",
       " 0.022538453,\n",
       " -0.00890809,\n",
       " 0.016513292,\n",
       " 0.030227914,\n",
       " -0.0025272253,\n",
       " -0.023901567,\n",
       " 0.031008158,\n",
       " 0.003681616,\n",
       " -0.014979488,\n",
       " 0.0006752768,\n",
       " 0.008934893,\n",
       " -0.010970884,\n",
       " 0.04827464,\n",
       " 0.028408349,\n",
       " 0.028016927,\n",
       " 0.0019551953,\n",
       " 0.08222381,\n",
       " -0.052067984,\n",
       " 0.006692697,\n",
       " -0.008836444,\n",
       " 0.013677494,\n",
       " -0.060968526,\n",
       " -0.011194318,\n",
       " -0.05064741,\n",
       " 0.0542971,\n",
       " 0.017583117,\n",
       " -0.08927467,\n",
       " 0.03703821,\n",
       " -0.015494282,\n",
       " -0.04923554,\n",
       " -0.03873719,\n",
       " 0.016617699,\n",
       " -0.0010309297,\n",
       " -0.020955889,\n",
       " -0.09268108,\n",
       " 0.008347074,\n",
       " -0.057232503,\n",
       " 0.014201132,\n",
       " -0.02310577,\n",
       " -0.008944738,\n",
       " -0.015635869,\n",
       " -0.004307023,\n",
       " -0.018662073,\n",
       " 0.049199246,\n",
       " 0.0031003957,\n",
       " 0.02640213,\n",
       " 0.0528915,\n",
       " -0.05751075,\n",
       " 0.010413189,\n",
       " 0.0008703783,\n",
       " -0.034134775,\n",
       " -0.0028080428,\n",
       " -0.023647891,\n",
       " -0.039262008,\n",
       " -0.04503833,\n",
       " 0.020042734,\n",
       " -0.02517899,\n",
       " 0.02110137,\n",
       " -0.0067599216,\n",
       " 0.02472751,\n",
       " -0.0138339335,\n",
       " -0.009423137,\n",
       " 0.016455462,\n",
       " 0.017229134,\n",
       " 0.022037424,\n",
       " 0.013708982,\n",
       " 0.07943137,\n",
       " 0.011363462,\n",
       " -0.007841985,\n",
       " -0.047680013,\n",
       " -0.03314241,\n",
       " -0.07041803,\n",
       " 0.040029,\n",
       " 0.021818206,\n",
       " 0.011685468,\n",
       " -0.02889787,\n",
       " -0.0043428573,\n",
       " -0.019936167,\n",
       " -0.019631209,\n",
       " -0.00243105,\n",
       " 0.002992529,\n",
       " 0.038940996,\n",
       " 0.013073643,\n",
       " 0.09731102,\n",
       " 0.0904998,\n",
       " -0.0048991623,\n",
       " 0.0048517585,\n",
       " -0.046781708,\n",
       " -0.026373489,\n",
       " 0.03496278,\n",
       " -0.024665434,\n",
       " 0.0066327727,\n",
       " 0.013456377,\n",
       " -0.024395294,\n",
       " -0.0038415082,\n",
       " -0.041797902,\n",
       " 0.061834514,\n",
       " 0.017155023,\n",
       " 0.053746942,\n",
       " -0.08612481,\n",
       " 0.080099225,\n",
       " -0.049392384,\n",
       " -0.028567495,\n",
       " 0.0053526447,\n",
       " -0.041502174,\n",
       " -0.035543855,\n",
       " -0.035468318,\n",
       " -0.007035466,\n",
       " -0.04765157,\n",
       " 0.0052381367,\n",
       " 0.070890285,\n",
       " -0.023822593,\n",
       " -0.028507536,\n",
       " -0.05583001,\n",
       " -0.04219448,\n",
       " 0.007039071,\n",
       " 0.033197343,\n",
       " 0.052984405,\n",
       " -0.034723464,\n",
       " -0.030927002,\n",
       " -0.020150544,\n",
       " 0.027416708,\n",
       " -0.06660975,\n",
       " 0.05215637,\n",
       " 0.03579222,\n",
       " -0.020689942,\n",
       " 0.0034690916,\n",
       " -0.035272487,\n",
       " 0.07161045,\n",
       " -0.012101815,\n",
       " -0.0011590549,\n",
       " -0.03406447,\n",
       " -0.060618676,\n",
       " 0.07619228,\n",
       " 0.036720507,\n",
       " 0.08401976,\n",
       " -0.049180213,\n",
       " 0.041981895,\n",
       " 0.03787731,\n",
       " -0.03132999,\n",
       " -0.06730224,\n",
       " -0.05848312,\n",
       " 0.0014715366,\n",
       " 0.063290514,\n",
       " -0.06117852,\n",
       " -0.007953283,\n",
       " 0.013666919,\n",
       " 0.001010048,\n",
       " 0.03772958,\n",
       " -0.0087128,\n",
       " -0.023071174,\n",
       " -0.01677302,\n",
       " 0.022280805,\n",
       " 0.015606181,\n",
       " 0.043852657,\n",
       " -0.020147564,\n",
       " -0.018769547,\n",
       " -0.047852684,\n",
       " 0.07760245,\n",
       " 0.019233612,\n",
       " 0.0006671538,\n",
       " -0.020011807,\n",
       " 0.045808617,\n",
       " -0.02569164,\n",
       " 0.031331606,\n",
       " 0.026198953,\n",
       " 0.0033233054,\n",
       " -0.046418425,\n",
       " 0.007661589,\n",
       " 0.057010107,\n",
       " 0.047272343,\n",
       " -0.021463484,\n",
       " 0.033206075,\n",
       " 0.011196667,\n",
       " 0.061814938,\n",
       " 0.024148725,\n",
       " -0.04611005,\n",
       " 0.007315372,\n",
       " -0.07058744,\n",
       " 0.0017460362,\n",
       " 0.03472592,\n",
       " -0.072797924,\n",
       " 0.0024288108,\n",
       " -0.051424943,\n",
       " -0.04007978,\n",
       " 0.020327935,\n",
       " -0.008609367,\n",
       " -0.025995446,\n",
       " 0.043286193,\n",
       " -0.032822873,\n",
       " 0.021684473,\n",
       " -0.012897724,\n",
       " -0.022262396,\n",
       " 0.028380902,\n",
       " 0.0049443883,\n",
       " -0.058694206,\n",
       " -0.009446026,\n",
       " -0.0042654476,\n",
       " -0.040142924,\n",
       " -0.019630587,\n",
       " -0.012544518,\n",
       " -0.03473777,\n",
       " -0.029415578,\n",
       " 0.0067038126,\n",
       " -0.047535636,\n",
       " 0.07348817,\n",
       " -0.063872285,\n",
       " 0.005572817,\n",
       " -0.030935915,\n",
       " -0.017581599,\n",
       " 0.038610745,\n",
       " -0.041056808,\n",
       " -0.04777762,\n",
       " -0.03953943,\n",
       " 0.051510427,\n",
       " -0.054574978,\n",
       " 0.05940309,\n",
       " 0.002136545,\n",
       " -0.018097682,\n",
       " 0.0147960195,\n",
       " 0.015963241,\n",
       " -0.038558315,\n",
       " -0.0025045692,\n",
       " 0.078088135,\n",
       " 0.018828463,\n",
       " 0.013820951,\n",
       " -0.015536992,\n",
       " 0.0697694,\n",
       " -0.0561698,\n",
       " 0.0074575683,\n",
       " 0.016196722,\n",
       " 0.01893447,\n",
       " -0.07461885,\n",
       " 0.019086694,\n",
       " 0.019609028,\n",
       " 0.051388856,\n",
       " -0.011081579,\n",
       " -0.017287664,\n",
       " 0.021272898,\n",
       " -0.014940176,\n",
       " -0.07653333,\n",
       " -0.05638581,\n",
       " -0.02902741,\n",
       " -0.02587279,\n",
       " -0.03658379,\n",
       " -0.034555648,\n",
       " -0.05031632,\n",
       " 0.019274537,\n",
       " -0.037117902,\n",
       " -0.018076872,\n",
       " 0.005341181,\n",
       " 0.01966482,\n",
       " -0.024089392,\n",
       " -0.023204865,\n",
       " 0.05337379,\n",
       " -0.008239011,\n",
       " 0.029877404,\n",
       " 0.010391888,\n",
       " -0.03254053,\n",
       " 0.03962414,\n",
       " -0.04759422,\n",
       " -0.08015142,\n",
       " -0.0065221037,\n",
       " -0.036348477,\n",
       " -0.052652087,\n",
       " 0.010819709,\n",
       " -0.011760265,\n",
       " 0.047083005,\n",
       " -0.0051238257,\n",
       " -0.002735475,\n",
       " 0.094833724,\n",
       " 0.021190882,\n",
       " -0.020473992,\n",
       " -0.01668503,\n",
       " 0.013581438,\n",
       " -0.016810311,\n",
       " 0.064227335,\n",
       " -0.030679617,\n",
       " 0.0060043004,\n",
       " -0.028180873,\n",
       " -0.02212965,\n",
       " 0.0075915093,\n",
       " -0.038194664,\n",
       " 0.028548041,\n",
       " -0.0051197535,\n",
       " -0.034531504,\n",
       " -0.015676534,\n",
       " -0.04785688,\n",
       " 0.023974668,\n",
       " -0.033942644,\n",
       " 0.07841377,\n",
       " -0.07195765,\n",
       " -0.0077972696,\n",
       " 0.06067054,\n",
       " 0.022187779,\n",
       " -0.00050693227,\n",
       " -0.009922473,\n",
       " 0.03610064,\n",
       " 0.043310124,\n",
       " -0.01985968,\n",
       " -0.012799419,\n",
       " 0.022741888,\n",
       " 0.012762802,\n",
       " -0.0007613162,\n",
       " -0.07373765,\n",
       " 0.0346901,\n",
       " -0.02094821,\n",
       " 0.0023991806,\n",
       " 0.015347652,\n",
       " 0.01873223,\n",
       " 0.0140542155,\n",
       " 0.017431656,\n",
       " 0.022375513,\n",
       " 0.009820195,\n",
       " -0.007591023,\n",
       " 0.006728598,\n",
       " -0.04039513,\n",
       " -0.04300423,\n",
       " -0.005141323,\n",
       " -0.04115019,\n",
       " -0.016391978,\n",
       " -0.044426404,\n",
       " 0.092481144,\n",
       " 0.011693248,\n",
       " -0.005577223,\n",
       " 0.050533716,\n",
       " -0.026961744,\n",
       " 0.04671717,\n",
       " 0.045148034,\n",
       " 0.010905261,\n",
       " -0.036454663,\n",
       " 0.01826605,\n",
       " 0.03098003,\n",
       " -0.031453524,\n",
       " -0.012921372,\n",
       " -0.037640996,\n",
       " 0.07210218,\n",
       " -0.055140786,\n",
       " -0.09676785,\n",
       " -0.06461354,\n",
       " 0.0063663647,\n",
       " -0.0050733862,\n",
       " -0.03924275,\n",
       " 0.018864475,\n",
       " -0.024150651,\n",
       " 0.002159536,\n",
       " 0.036933,\n",
       " -0.016511302,\n",
       " -0.0017240186,\n",
       " -0.055266865,\n",
       " 0.03066881,\n",
       " 0.053876363,\n",
       " 0.016960798,\n",
       " 0.015904572,\n",
       " -0.05225233,\n",
       " -0.023517542,\n",
       " -0.040703747,\n",
       " 0.022439087,\n",
       " 0.04142266,\n",
       " 0.003706908,\n",
       " -0.059816618,\n",
       " -0.0090390965,\n",
       " 0.016800143,\n",
       " 0.04317265,\n",
       " -0.006624565,\n",
       " -0.01094222,\n",
       " 0.05609629,\n",
       " 0.029930329,\n",
       " 0.0082404325,\n",
       " -0.010766454,\n",
       " -0.007510313,\n",
       " -0.021575915,\n",
       " -0.06957331,\n",
       " -0.039343007,\n",
       " 0.036298007,\n",
       " 0.05634237,\n",
       " -0.020538813,\n",
       " 0.00072596705,\n",
       " -0.007819527,\n",
       " -0.037452083,\n",
       " 0.011770736,\n",
       " 0.0071832524,\n",
       " -0.029955048,\n",
       " -0.0383469,\n",
       " -0.029061018,\n",
       " -0.040066767,\n",
       " -0.028999036,\n",
       " 0.014099888,\n",
       " -0.05707727,\n",
       " 0.046688676,\n",
       " 0.041055165,\n",
       " -0.022599604,\n",
       " -0.009232065,\n",
       " -0.03730886,\n",
       " 0.011315334,\n",
       " -0.04234891,\n",
       " 0.006154338,\n",
       " 0.009979354,\n",
       " -0.0115151405,\n",
       " -0.0022406138,\n",
       " 0.0050966647,\n",
       " -0.010724652,\n",
       " -0.029302059,\n",
       " 0.015086459,\n",
       " 0.037326645,\n",
       " -0.037722934,\n",
       " -0.027999306,\n",
       " 0.002054682,\n",
       " -0.019531023,\n",
       " 0.021826679,\n",
       " 0.01335013,\n",
       " 0.061977576,\n",
       " -0.026266273,\n",
       " -0.001383621,\n",
       " 0.024383917,\n",
       " 0.016468173,\n",
       " -0.030417958,\n",
       " 0.045348503,\n",
       " -0.009365673,\n",
       " -0.037168585,\n",
       " -0.008830825,\n",
       " 0.008700784,\n",
       " -0.0047239554,\n",
       " -0.035261232,\n",
       " 0.0014338495,\n",
       " -0.056039482,\n",
       " 0.02366757,\n",
       " -0.014547044,\n",
       " -0.009957398,\n",
       " 0.008942947,\n",
       " -0.0133502865,\n",
       " -0.0426489,\n",
       " -0.024932139,\n",
       " -0.007827192,\n",
       " 0.013504213,\n",
       " -0.034219757,\n",
       " -0.050648443,\n",
       " -0.0002492067,\n",
       " 0.015462756,\n",
       " -0.023344686,\n",
       " 0.095592365,\n",
       " -0.022502124,\n",
       " -0.009147129,\n",
       " -0.02537602,\n",
       " 0.026414482,\n",
       " -0.0356212,\n",
       " -0.008277952,\n",
       " 0.0020022746,\n",
       " 0.07916894,\n",
       " 0.029794183,\n",
       " -0.004269543,\n",
       " 0.04605732,\n",
       " -0.005474296,\n",
       " -0.028887263,\n",
       " -0.0067601637,\n",
       " 0.0034494787,\n",
       " 0.026154868,\n",
       " -0.04673974,\n",
       " -0.0010432772,\n",
       " 0.082373105,\n",
       " 0.0023341351,\n",
       " 0.008737902,\n",
       " -0.012416494,\n",
       " -0.020349203,\n",
       " 0.011337629,\n",
       " 0.0024089336,\n",
       " -0.017137446,\n",
       " -0.0011846739,\n",
       " 0.016057301,\n",
       " -0.026486484,\n",
       " 0.007632168,\n",
       " -0.0067941174,\n",
       " -0.022311114,\n",
       " 0.0025623548,\n",
       " 0.0010408678,\n",
       " 0.01899918,\n",
       " 0.039652582,\n",
       " -0.0064878007,\n",
       " -0.086033344,\n",
       " -0.0018461132,\n",
       " -0.053914495,\n",
       " 0.020866781,\n",
       " -0.020268654,\n",
       " -0.010626602,\n",
       " -0.04104165,\n",
       " -0.034751195,\n",
       " 0.020628123,\n",
       " 0.02621603,\n",
       " -0.037797164,\n",
       " -2.7846123e-05,\n",
       " 0.035927083,\n",
       " 0.008578445,\n",
       " 0.043649282,\n",
       " 0.056697994,\n",
       " -0.057602383,\n",
       " -0.07114319,\n",
       " -0.029277707,\n",
       " -0.043005865,\n",
       " -0.0330038,\n",
       " -0.03702514,\n",
       " -0.036795825,\n",
       " 0.00851171,\n",
       " -0.031004295,\n",
       " 0.036411855,\n",
       " -0.047726173,\n",
       " -0.00018352593,\n",
       " -0.004504276,\n",
       " -0.003696735,\n",
       " -0.012019336,\n",
       " -0.07379269,\n",
       " -0.037272107,\n",
       " -0.0023374865,\n",
       " 0.030088283,\n",
       " 0.025915034,\n",
       " 0.019723253,\n",
       " -0.06462367,\n",
       " -0.017263917,\n",
       " 0.0004014133,\n",
       " 0.022456272,\n",
       " 0.0042809616,\n",
       " -0.020144518,\n",
       " -0.039429855,\n",
       " -0.0036597627,\n",
       " 0.024795255,\n",
       " 0.043570895,\n",
       " -0.010890538,\n",
       " 0.024411708,\n",
       " -0.026934959,\n",
       " -0.014071236,\n",
       " 0.049432043,\n",
       " -0.03213929,\n",
       " 0.03896862,\n",
       " 0.0059318244,\n",
       " 0.034887746,\n",
       " 0.06649252,\n",
       " -0.0677867,\n",
       " 0.02209373,\n",
       " 0.025173584,\n",
       " -0.013883227,\n",
       " -0.016571458,\n",
       " 0.035387088,\n",
       " 0.08048465,\n",
       " 0.016191175,\n",
       " 0.052293546,\n",
       " -0.07032631,\n",
       " -0.022504201,\n",
       " 0.019322656,\n",
       " 0.055986147,\n",
       " -0.0042245127,\n",
       " -0.013066456,\n",
       " -0.01565691,\n",
       " -0.003948672,\n",
       " 0.020220611,\n",
       " -0.025177369,\n",
       " 0.0059597325,\n",
       " -0.02773647,\n",
       " 0.04065444,\n",
       " 0.007431184,\n",
       " -0.034045897,\n",
       " -0.00010900676,\n",
       " 0.02743978,\n",
       " 0.024593081,\n",
       " 0.018604005,\n",
       " 0.009718282,\n",
       " 0.014675875,\n",
       " 0.005164459,\n",
       " -0.008161345,\n",
       " -0.0520892,\n",
       " 0.056443207,\n",
       " 0.06391053,\n",
       " 0.0064486396,\n",
       " -0.017517237,\n",
       " -0.024063218,\n",
       " 0.001490828,\n",
       " -0.026171852,\n",
       " -0.03700921,\n",
       " 0.06996642,\n",
       " -0.038176518,\n",
       " -0.026842874,\n",
       " 0.01862549,\n",
       " -0.00016371439,\n",
       " -0.002758814,\n",
       " 0.03257906,\n",
       " -0.0032778145,\n",
       " 0.07953606,\n",
       " -0.043802284,\n",
       " -0.007847629,\n",
       " 0.010230062,\n",
       " -0.055474363,\n",
       " 0.016397005,\n",
       " 0.0336693,\n",
       " 0.003863022,\n",
       " -0.07070299,\n",
       " -0.010618192,\n",
       " -0.051352117,\n",
       " 0.02294336,\n",
       " -0.0345636,\n",
       " -0.004798472,\n",
       " -0.031282194,\n",
       " -0.020657377,\n",
       " -0.015997216,\n",
       " 0.008446628,\n",
       " -0.011032261,\n",
       " -0.036930837,\n",
       " 0.019097641,\n",
       " 0.008496695,\n",
       " 0.035815928,\n",
       " -0.025694035,\n",
       " -0.037762187,\n",
       " 0.036297396,\n",
       " 0.015417715,\n",
       " 0.0068301135,\n",
       " 0.041146312,\n",
       " 0.00023672439,\n",
       " -0.014160575,\n",
       " 0.006608217,\n",
       " -0.007494661,\n",
       " 0.041601893,\n",
       " -0.021609476,\n",
       " 0.056323994,\n",
       " -0.0042110938,\n",
       " 0.023522215,\n",
       " 0.014042903,\n",
       " 0.09015395,\n",
       " 0.06589186,\n",
       " -0.06791596,\n",
       " -0.003769355,\n",
       " 0.060750283,\n",
       " -0.017901408,\n",
       " -0.0018718097,\n",
       " -0.025072312,\n",
       " -0.022840358,\n",
       " -0.013333188,\n",
       " 0.050132398,\n",
       " -0.014995724,\n",
       " 0.022651779,\n",
       " 0.05636576,\n",
       " 0.0063988264,\n",
       " 0.024092497,\n",
       " 0.011631764,\n",
       " 0.0016865643,\n",
       " -0.030254085,\n",
       " -0.027695423,\n",
       " -0.022065327,\n",
       " 0.0053252922,\n",
       " 0.013827252,\n",
       " -0.032449745,\n",
       " 0.028143734,\n",
       " 0.0020643978,\n",
       " 0.015793547,\n",
       " -0.026526645,\n",
       " -0.018549897,\n",
       " 0.036002893,\n",
       " 0.021218412,\n",
       " 0.010027808,\n",
       " -0.02489813,\n",
       " -0.040378086,\n",
       " 0.023610303,\n",
       " 0.021460062,\n",
       " 0.02587277,\n",
       " 0.10630414,\n",
       " -0.018111626,\n",
       " -0.053478498,\n",
       " 0.0066925394,\n",
       " -0.048720744,\n",
       " 0.028061964,\n",
       " 0.020732868,\n",
       " 0.029830314,\n",
       " 0.024977192,\n",
       " -0.021619737,\n",
       " 0.038610324,\n",
       " -0.016060581,\n",
       " 0.048272125,\n",
       " 0.028830662,\n",
       " -0.007011714,\n",
       " 0.014937948,\n",
       " 0.025430018,\n",
       " -0.017769743,\n",
       " 0.042946726,\n",
       " -0.029492993,\n",
       " -0.027637552,\n",
       " 0.014294798,\n",
       " 0.009775252,\n",
       " -0.029866775,\n",
       " 0.016967783,\n",
       " -0.033851378,\n",
       " -0.011973757]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_content = \"\"\"\n",
    "# nomic-embed-text-v2-moe is a multilingual MoE text embedding model that excels at multilingual retrieval.\n",
    "\n",
    "# High Performance: SoTA Multilingual performance compared to ~300M parameter models, competitive with models 2x in size\n",
    "# Multilinguality: Supports ~100 languages and trained on over 1.6B pairs\n",
    "# Flexible Embedding Dimension: Trained with Matryoshka Embeddings with 3x reductions in storage cost with minimal performance degradations\n",
    "# Fully Open-Source: Model weights, code, and training data\n",
    "# nomic-embed-text-v2-moe is a multilingual MoE text embedding model that excels at multilingual retrieval.\n",
    "\n",
    "# High Performance: SoTA Multilingual performance compared to ~300M parameter models, competitive with models 2x in size\n",
    "# Multilinguality: Supports ~100 languages and trained on over 1.6B pairs\n",
    "# Flexible Embedding Dimension: Trained with Matryoshka Embeddings with 3x reductions in storage cost with minimal performance degradations\n",
    "# Fully Open-Source: Model weights, code, and training data\n",
    "# nomic-embed-text-v2-moe is a multilingual MoE text embedding model that excels at multilingual retrieval.\n",
    "\n",
    "# High Performance: SoTA Multilingual performance compared to ~300M parameter models, competitive with models 2x in size\n",
    "# Multilinguality: Supports ~100 languages and trained on over 1.6B pairs\n",
    "# Flexible Embedding Dimension: Trained with Matryoshka Embeddings with 3x reductions in storage cost with minimal performance degradations\n",
    "# Fully Open-Source: Model weights, code, and training data\n",
    "# nomic-embed-text-v2-moe is a multilingual MoE text embedding model that excels at multilingual retrieval.\n",
    "\n",
    "# High Performance: SoTA Multilingual performance compared to ~300M parameter models, competitive with models 2x in size\n",
    "# Multilinguality: Supports ~100 languages and trained on over 1.6B pairs\n",
    "# Flexible Embedding Dimension: Trained with Matryoshka Embeddings with 3x reductions in storage cost with minimal performance degradations\n",
    "# Fully Open-Source: Model weights, code, and training data\n",
    "# nomic-embed-text-v2-moe is a multilingual MoE text embedding model that excels at multilingual retrieval.\n",
    "\n",
    "# High Performance: SoTA Multilingual performance compared to ~300M parameter models, competitive with models 2x in size\n",
    "# Multilinguality: Supports ~100 languages and trained on over 1.6B pairs\n",
    "# Flexible Embedding Dimension: Trained with Matryoshka Embeddings with 3x reductions in storage cost with minimal performance degradations\n",
    "# Fully Open-Source: Model weights, code, and training data\n",
    "# \"\"\"\n",
    "\n",
    "# vector = embeddings.embed_query(text_content)\n",
    "# vector.__len__()\n",
    "\n",
    "# len(vector)\n",
    "\n",
    "# vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c996a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "index = 768\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "071f55f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.25',\n",
      " 'creator': 'LaTeX with hyperref',\n",
      " 'creationdate': '2025-04-29T01:11:16+00:00',\n",
      " 'author': '',\n",
      " 'keywords': '',\n",
      " 'moddate': '2025-04-29T01:11:16+00:00',\n",
      " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live '\n",
      "                    '2023) kpathsea version 6.3.5',\n",
      " 'subject': '',\n",
      " 'title': '',\n",
      " 'trapped': '/False',\n",
      " 'source': './Mem0.pdf',\n",
      " 'total_pages': 23,\n",
      " 'page': 0,\n",
      " 'page_label': '1'}\n",
      "\n",
      "Mem0: Building Production-Ready AI Agents with\n",
      "Scalable Long-Term Memory\n",
      "Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh,and Deshraj Yadav\n",
      "research@mem0.ai\n",
      "Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent\n",
      "responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over\n",
      "prolonged multi-session dialogues. We introduceMem0, a scalable memory-centric architecture that addresses\n",
      "this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conver-\n",
      "sations. Building on this foundation, we further propose an enhanced variant that leverages graph-based\n",
      "memory representations to capture complex relational structures among conversational elements. Through\n",
      "comprehensive evaluations on theLOCOMO benchmark, we systematically compare our approaches against six\n",
      "baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG)\n",
      "with varying chunk sizes andk-values, (iii) a full-context approach that processes the entire conversation\n",
      "history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory\n",
      "management platform. Empirical results demonstrate that our methods consistently outperform all existing\n",
      "memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. No-\n",
      "tably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, whileMem0 with\n",
      "graph memory achieves around 2% higher overall score than the baseMem0 configuration. Beyond accuracy\n",
      "gains, we also markedly reduce computational overhead compared to the full-context approach. In particular,\n",
      "Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, thereby offering a compelling\n",
      "balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight\n",
      "the critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving\n",
      "the way for more reliable and efficient LLM-driven AI agents.\n",
      "Code can be found at: https://mem0.ai/research\n",
      "1. Introduction\n",
      "Human memory is afoundation of intelligence—it shapes our identity, guides decision-making, and enables\n",
      "us to learn, adapt, and form meaningful relationships (Craik and Jennings, 1992). Among its many roles,\n",
      "memory is essential for communication: we recall past interactions, infer preferences, and construct evolving\n",
      "mental models of those we engage with (Assmann, 2011). This ability to retain and retrieve information\n",
      "over extended periods enables coherent, contextually rich exchanges that span days, weeks, or even months.\n",
      "AI agents, powered by large language models (LLMs), have made remarkable progress in generating fluent,\n",
      "contextually appropriate responses (Yu et al., 2024, Zhang et al., 2024). However, these systems are\n",
      "fundamentally limited by their reliance on fixed context windows, which severely restrict their ability to\n",
      "maintain coherence over extended interactions (Bulatov et al., 2022, Liu et al., 2023). This limitation stems\n",
      "from LLMs’ lack of persistent memory mechanisms that can extend beyond their finite context windows.\n",
      "While humans naturally accumulate and organize experiences over time, forming a continuous narrative\n",
      "arXiv:2504.19413v1  [cs.CL]  28 Apr 2025\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./Mem0.pdf\"\n",
    "loader = PyPDFLoader(file_path, mode=\"page\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "pprint.pp(docs[0].metadata)\n",
    "\n",
    "print()\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2c8dd220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "53114682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id1',\n",
       " 'id2',\n",
       " 'id3',\n",
       " 'id4',\n",
       " 'id5',\n",
       " 'id6',\n",
       " 'id7',\n",
       " 'id8',\n",
       " 'id9',\n",
       " 'id10',\n",
       " 'id11',\n",
       " 'id12',\n",
       " 'id13',\n",
       " 'id14',\n",
       " 'id15',\n",
       " 'id16',\n",
       " 'id17',\n",
       " 'id18',\n",
       " 'id19',\n",
       " 'id20',\n",
       " 'id21',\n",
       " 'id22',\n",
       " 'id23']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(\n",
    "    documents=docs, ids=[f\"id{num + 1}\" for num in range(0, len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d3ac7964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\n",
      "Table 1:Performance comparison of memory-enabled systems across different question types in theLOCOMO dataset.\n",
      "Evaluation metrics include F1 score (F1), BLEU-1 (B1), and LLM-as-a-Judge score (J), with higher values indicating\n",
      "better performance.A-Mem∗ represents results from our re-run of A-Mem to generate LLM-as-a-Judge scores by setting\n",
      "temperature as 0.Mem0g indicates our proposed architecture enhanced with graph memory.Bold denotes the best\n",
      "performance for each metric across all methods. (↑) represents higher score is better.\n",
      "Method Single Hop Multi-Hop Open Domain Temporal\n",
      "F1 ↑ B1 ↑ J↑ F1 ↑ B1 ↑ J↑ F1 ↑ B1 ↑ J↑ F1 ↑ B1 ↑ J↑\n",
      "LoCoMo 25.02 19.75 – 12.04 11.16 – 40.36 29.05 – 18.41 14.77 –\n",
      "ReadAgent 9.15 6.48 – 5.31 5.12 – 9.67 7.66 – 12.60 8.87 –\n",
      "MemoryBank5.00 4.77 – 5.56 5.94 – 6.61 5.16 – 9.68 6.99 –\n",
      "MemGPT 26.65 17.72 – 9.15 7.44 – 41.04 34.34 – 25.52 19.44 –\n",
      "A-Mem 27.02 20.09 – 12.14 12.00 – 44.65 37.06 – 45.85 36.67 –\n",
      "A-Mem* 20.76 14.90 39.79±0.38 9.22 8.81 18.85±0.31 33.34 27.58 54.05±0.22 35.40 31.08 49.91±0.31\n",
      "LangMem 35.51 26.86 62.23±0.75 26.0422.3247.92±0.47 40.91 33.63 71.12±0.20 30.75 25.84 23.43±0.39\n",
      "Zep 35.74 23.30 61.70±0.32 19.37 14.82 41.35±0.48 49.5638.9276.60±0.13 42.00 34.53 49.31±0.50\n",
      "OpenAI 34.30 23.72 63.79±0.46 20.09 15.42 42.92±0.63 39.31 31.16 62.29±0.12 14.04 11.25 21.71±0.20\n",
      "Mem0 38.72 27.13 67.13±0.6528.6421.5851.15±0.31 47.65 38.72 72.93±0.11 48.9340.5155.51±0.34\n",
      "Mem0g 38.09 26.03 65.71±0.45 24.32 18.82 47.19±0.67 49.2740.3075.71±0.21 51.5540.2858.13±0.44\n",
      "text. These generated memories are then used as complete context for answering questions about each\n",
      "conversation, intentionally granting the OpenAI approach privileged access to all memories rather than\n",
      "only question-relevant ones. This methodology accommodates the lack of external API access for selective\n",
      "memory retrieval in OpenAI’s system for benchmarking.\n",
      "MemoryProviders WeincorporateZep(Rasmussenetal.,2025),amemorymanagementplatformdesigned\n",
      "for AI agents. Using their platform version, we conduct systematic evaluations across theLOCOMO dataset,\n",
      "maintaining temporal fidelity by preserving timestamp information alongside conversational content. This\n",
      "temporalanchoringensuresthattime-sensitivequeriescanbeaddressedthroughappropriatelycontextualized\n",
      "memory retrieval, particularly important for evaluating questions that require chronological awareness.\n",
      "This baseline represents an important commercial implementation of memory management specifically\n",
      "engineered for AI agents.\n",
      "4. Evaluation Results, Analysis and Discussion.\n",
      "4.1. Performance Comparison Across Memory-Enabled Systems\n",
      "Table1reportsF 1, B1 andJscoresforourtwoarchitectures— Mem0and Mem0g —againstasuiteofcompetitive\n",
      "baselines, asmentionedinSection3, onsingle -hop, multi-hop, open-domain, andtemporalquestions. Overall,\n",
      "both of our models set new state-of-the-art marks in all the three evaluation metrics for most question types.\n",
      "Single-Hop Question Performance Single-hop queries involve locating a single factual span contained\n",
      "within one dialogue turn. Leveraging its dense memories in natural language text,Mem0 secures the strongest\n",
      "results:F1=38.72, B1=27.13, and J=67.13. Augmenting the natural language memories with graph memory\n",
      "(Mem0g) yields marginal performance drop compared toMem0, indicating that relational structure provides\n",
      "limited utility when the retrieval target occupies a single turn. Among the existing baselines, the full-context\n",
      "OpenAIrun attains the next-best J score, reflecting the benefits of retaining the entire conversation in context,\n",
      "while LangMem and Zep both score around 8% relatively less against our models on J score. PreviousLOCOMO\n",
      "9\n",
      "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\n",
      "B. Algorithm\n",
      "Algorithm 1Memory Management System: Update Operations\n",
      "1: Input: Set of retrieved memoriesF, Existing memory storeM = {m1, m2, . . . , mn}\n",
      "2: Output: Updated memory storeM′\n",
      "3: procedure Up dat e M e mory(F, M)\n",
      "4: for each fact f ∈ F do\n",
      "5: operation ← C l as s i f y Op e r at ion(f , M) ▷ Execute appropriate operation based on\n",
      "classification\n",
      "6: if operation = ADDthen\n",
      "7: id ← GenerateUniqueID()\n",
      "8: M ← M ∪ {(id, f ,\"ADD\")} ▷ Add new fact with unique identifier\n",
      "9: else if operation = UPDATEthen\n",
      "10: mi ← FindRelatedMemory(f , M)\n",
      "11: if InformationContent(f ) > InformationContent(mi)then\n",
      "12: M ← (M \\ {mi})∪ {(idi, f ,\"UPDATE\")} ▷ Replace with richer information\n",
      "13: end if\n",
      "14: else if operation = DELETE then\n",
      "15: mi ← FindContradictedMemory(f , M)\n",
      "16: M ← M \\ {mi} ▷ Remove contradicted information\n",
      "17: else if operation = NOOPthen\n",
      "18: No operation performed ▷ Fact already exists or is irrelevant\n",
      "19: end if\n",
      "20: end for\n",
      "21: return M\n",
      "22: end procedure\n",
      "23: function C l as s i f y Op e r at ion( f , M)\n",
      "24: if ¬SemanticallySimilar(f , M)then\n",
      "25: return ADD ▷ New information not present in memory\n",
      "26: else ifContradicts(f , M)then\n",
      "27: return DELETE ▷ Information conflicts with existing memory\n",
      "28: else ifAugments(f , M)then\n",
      "29: return UPDATE ▷ Enhances existing information in memory\n",
      "30: else\n",
      "31: return NOOP ▷ No change required\n",
      "32: end if\n",
      "33: end function\n",
      "C. Selected Baselines\n",
      "LoCoMo The LoCoMo framework implements a sophisticated memory pipeline that enables LLM agents to\n",
      "maintain coherent, long-term conversations. At its core, the system divides memory into short-term and\n",
      "long-term components. After each conversation session, agents generate summaries (stored as short-term\n",
      "memory) that distill key information from that interaction. Simultaneously, individual conversation turns are\n",
      "21\n",
      "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\n",
      "Table 2:Performance comparison of various baselines with proposed methods. Latency measurements show p50\n",
      "(median) and p95 (95th percentile) values in seconds for both search time (time taken to fetch memories/chunks) and\n",
      "total time (time to generate the complete response). Overall LLM-as-a-Judge score (J) represents the quality metric of\n",
      "the generated responses on the entireLOCOMO dataset.\n",
      "Method\n",
      "Latency (seconds) Overall\n",
      "JSearch Total\n",
      "K chunk size/\n",
      "memory tokens p50 p95 p50 p95\n",
      "RAG\n",
      "1\n",
      "128 0.281 0.823 0.774 1.825 47.77 ± 0.23%\n",
      "256 0.251 0.710 0.745 1.628 50.15 ± 0.16%\n",
      "512 0.240 0.639 0.772 1.710 46.05 ± 0.14%\n",
      "1024 0.240 0.723 0.821 1.957 40.74 ± 0.17%\n",
      "2048 0.255 0.752 0.996 2.182 37.93 ± 0.12%\n",
      "4096 0.254 0.719 1.093 2.711 36.84 ± 0.17%\n",
      "8192 0.279 0.838 1.396 4.416 44.53 ± 0.13%\n",
      "2\n",
      "128 0.267 0.624 0.766 1.829 59.56 ± 0.19%\n",
      "256 0.255 0.699 0.802 1.907 60.97 ± 0.20%\n",
      "512 0.247 0.746 0.829 1.729 58.19 ± 0.18%\n",
      "1024 0.238 0.702 0.860 1.850 50.68 ± 0.13%\n",
      "2048 0.261 0.829 1.101 2.791 48.57 ± 0.22%\n",
      "4096 0.266 0.944 1.451 4.822 51.79 ± 0.15%\n",
      "8192 0.288 1.124 2.312 9.942 60.53 ± 0.16%\n",
      "Full-context 26031 - - 9.870 17.117 72.90 ± 0.19%\n",
      "A-Mem 2520 0.668 1.485 1.410 4.374 48.38 ± 0.15%\n",
      "LangMem 127 17.99 59.82 18.53 60.40 58.10 ± 0.21%\n",
      "Zep 3911 0.513 0.778 1.292 2.926 65.99 ± 0.16%\n",
      "OpenAI 4437 - - 0.466 0.889 52.90 ± 0.14%\n",
      "Mem0 1764 0.148 0.200 0.708 1.440 66.88 ± 0.15%\n",
      "Mem0g 3616 0.476 0.657 1.091 2.590 68.44 ± 0.17%\n",
      "expected relational advantages ofMem0g do not translate into better outcomes here, suggesting potential\n",
      "overhead or redundancy when navigating more intricate graph structures in multi-step reasoning scenarios.\n",
      "In temporal reasoning, Mem0g substantially outperforms other methods, validating that structured\n",
      "relational graphs excel in capturing chronological relationships and event sequences. The presence of explicit\n",
      "relational context significantly enhancesMem0g’s temporal coherence, outperformingMem0’s dense memory\n",
      "storage and highlighting the importance of precise relational representations when tracking temporally\n",
      "sensitive information. Open-domain performance further reinforces the value of relational modeling.Mem0g,\n",
      "benefiting from the relational clarity of graph-based memory, closely competes with the top-performing\n",
      "baseline (Zep). This competitive result underscoresMem0g’s robustness in integrating external knowledge\n",
      "through relational clarity, suggesting an optimal synergy between structured memory and open-domain\n",
      "information synthesis.\n",
      "11\n",
      "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\n",
      "robustframeworkforevaluatingtheeffectivenessofdifferentmemoryarchitecturesacrossvariousdimensions,\n",
      "including factual accuracy, computational efficiency, and scalability to extended conversations. Where\n",
      "applicable, unless otherwise specified, we set the temperature to 0 to ensure the runs are as reproducible as\n",
      "possible.\n",
      "Established LOCOMO Benchmarks We first establish a comparative foundation by evaluating previously\n",
      "benchmarked methods on theLOCOMO dataset. These include five established approaches: LoCoMo (Maha-\n",
      "rana et al., 2024), ReadAgent (Lee et al., 2024), MemoryBank (Zhong et al., 2024), MemGPT (Packer et al.,\n",
      "2023), and A-Mem (Xu et al., 2025). These established benchmarks not only provide direct comparison\n",
      "points with published results but also represent the evolution of conversational memory architectures across\n",
      "different algorithmic paradigms. For our evaluation, we select the metrics wheregpt-4o-mini was used\n",
      "for the evaluation. More details about these benchmarks are mentioned in Appendix C.\n",
      "Open-Source Memory Solutions Our second category consists of promising open-source memory architec-\n",
      "tures such as LangMem2 (Hot Path) that have demonstrated effectiveness in related conversational tasks but\n",
      "have not yet been evaluated on theLOCOMO dataset. By adapting these systems to our evaluation framework,\n",
      "we broaden the comparative landscape and identify potential alternative approaches that may offer competi-\n",
      "tive performance. We initialized the LLM withgpt-4o-mini and usedtext-embedding-small-3 as the\n",
      "embedding model.\n",
      "Retrieval-Augmented Generation (RAG) As a baseline, we treat the entire conversation history as a\n",
      "document collection and apply a standard RAG pipeline. We first segment each conversation into fixed-length\n",
      "chunks (128, 256, 512, 1024, 2048, 4096, and 8192 tokens), where 8192 is the maximum chunk size\n",
      "supported by our embedding model. All chunks are embedded using OpenAI’stext-embedding-small-3\n",
      "to ensure consistent vector quality across configurations. At query time, we retrieve the topk chunks by\n",
      "semantic similarity and concatenate them as context for answer generation. Throughout our experiments we\n",
      "set k∈{1,2}: with k=1 only the single most relevant chunk is used, and withk=2 the two most relevant\n",
      "chunks (up to 16384 tokens) are concatenated. We avoidk > 2 since the average conversation length (26000\n",
      "tokens) would be fully covered, negating the benefits of selective retrieval. By varying chunk size andk, we\n",
      "systematically evaluate RAG performance on long-term conversational memory tasks.\n",
      "Full-Context Processing We adopt a straightforward approach by passing the entire conversation history\n",
      "within the context window of the LLM. This method leverages the model’s inherent ability to process\n",
      "sequentialinformationwithoutadditionalarchitecturalcomponents. Whileconceptuallysimple,thisapproach\n",
      "faces practical limitations as conversation length increases, eventually increasing token cost and latency.\n",
      "Nevertheless, it establishes an important reference point for understanding the value of more sophisticated\n",
      "memory mechanisms compared to direct processing of available context.\n",
      "ProprietaryModels WeevaluateOpenAI’smemory3 featureavailableintheirChatGPTinterface,specifically\n",
      "using gpt-4o-mini for consistency. We ingest entireLOCOMO conversations with a prompt (see Appendix A)\n",
      "intosinglechatsessions,promptingmemorygenerationwithtimestamps,participantnames,andconversation\n",
      "2https://langchain-ai.github.io/langmem/\n",
      "3https://openai.com/index/memory-and-new-controls-for-chatgpt/\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "similar_docs = vector_store.similarity_search(\"LOCOMO\")\n",
    "\n",
    "for doc in similar_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "88fe36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(prompt):\n",
    "    context = vector_store.similarity_search(prompt)\n",
    "    # print(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "956ce939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='id9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:11:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:11:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './Mem0.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9'}, page_content='Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\\nTable 1:Performance comparison of memory-enabled systems across different question types in theLOCOMO dataset.\\nEvaluation metrics include F1 score (F1), BLEU-1 (B1), and LLM-as-a-Judge score (J), with higher values indicating\\nbetter performance.A-Mem∗ represents results from our re-run of A-Mem to generate LLM-as-a-Judge scores by setting\\ntemperature as 0.Mem0g indicates our proposed architecture enhanced with graph memory.Bold denotes the best\\nperformance for each metric across all methods. (↑) represents higher score is better.\\nMethod Single Hop Multi-Hop Open Domain Temporal\\nF1 ↑ B1 ↑ J↑ F1 ↑ B1 ↑ J↑ F1 ↑ B1 ↑ J↑ F1 ↑ B1 ↑ J↑\\nLoCoMo 25.02 19.75 – 12.04 11.16 – 40.36 29.05 – 18.41 14.77 –\\nReadAgent 9.15 6.48 – 5.31 5.12 – 9.67 7.66 – 12.60 8.87 –\\nMemoryBank5.00 4.77 – 5.56 5.94 – 6.61 5.16 – 9.68 6.99 –\\nMemGPT 26.65 17.72 – 9.15 7.44 – 41.04 34.34 – 25.52 19.44 –\\nA-Mem 27.02 20.09 – 12.14 12.00 – 44.65 37.06 – 45.85 36.67 –\\nA-Mem* 20.76 14.90 39.79±0.38 9.22 8.81 18.85±0.31 33.34 27.58 54.05±0.22 35.40 31.08 49.91±0.31\\nLangMem 35.51 26.86 62.23±0.75 26.0422.3247.92±0.47 40.91 33.63 71.12±0.20 30.75 25.84 23.43±0.39\\nZep 35.74 23.30 61.70±0.32 19.37 14.82 41.35±0.48 49.5638.9276.60±0.13 42.00 34.53 49.31±0.50\\nOpenAI 34.30 23.72 63.79±0.46 20.09 15.42 42.92±0.63 39.31 31.16 62.29±0.12 14.04 11.25 21.71±0.20\\nMem0 38.72 27.13 67.13±0.6528.6421.5851.15±0.31 47.65 38.72 72.93±0.11 48.9340.5155.51±0.34\\nMem0g 38.09 26.03 65.71±0.45 24.32 18.82 47.19±0.67 49.2740.3075.71±0.21 51.5540.2858.13±0.44\\ntext. These generated memories are then used as complete context for answering questions about each\\nconversation, intentionally granting the OpenAI approach privileged access to all memories rather than\\nonly question-relevant ones. This methodology accommodates the lack of external API access for selective\\nmemory retrieval in OpenAI’s system for benchmarking.\\nMemoryProviders WeincorporateZep(Rasmussenetal.,2025),amemorymanagementplatformdesigned\\nfor AI agents. Using their platform version, we conduct systematic evaluations across theLOCOMO dataset,\\nmaintaining temporal fidelity by preserving timestamp information alongside conversational content. This\\ntemporalanchoringensuresthattime-sensitivequeriescanbeaddressedthroughappropriatelycontextualized\\nmemory retrieval, particularly important for evaluating questions that require chronological awareness.\\nThis baseline represents an important commercial implementation of memory management specifically\\nengineered for AI agents.\\n4. Evaluation Results, Analysis and Discussion.\\n4.1. Performance Comparison Across Memory-Enabled Systems\\nTable1reportsF 1, B1 andJscoresforourtwoarchitectures— Mem0and Mem0g —againstasuiteofcompetitive\\nbaselines, asmentionedinSection3, onsingle -hop, multi-hop, open-domain, andtemporalquestions. Overall,\\nboth of our models set new state-of-the-art marks in all the three evaluation metrics for most question types.\\nSingle-Hop Question Performance Single-hop queries involve locating a single factual span contained\\nwithin one dialogue turn. Leveraging its dense memories in natural language text,Mem0 secures the strongest\\nresults:F1=38.72, B1=27.13, and J=67.13. Augmenting the natural language memories with graph memory\\n(Mem0g) yields marginal performance drop compared toMem0, indicating that relational structure provides\\nlimited utility when the retrieval target occupies a single turn. Among the existing baselines, the full-context\\nOpenAIrun attains the next-best J score, reflecting the benefits of retaining the entire conversation in context,\\nwhile LangMem and Zep both score around 8% relatively less against our models on J score. PreviousLOCOMO\\n9'),\n",
       " Document(id='id21', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:11:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:11:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './Mem0.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21'}, page_content='Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\\nB. Algorithm\\nAlgorithm 1Memory Management System: Update Operations\\n1: Input: Set of retrieved memoriesF, Existing memory storeM = {m1, m2, . . . , mn}\\n2: Output: Updated memory storeM′\\n3: procedure Up dat e M e mory(F, M)\\n4: for each fact f ∈ F do\\n5: operation ← C l as s i f y Op e r at ion(f , M) ▷ Execute appropriate operation based on\\nclassification\\n6: if operation = ADDthen\\n7: id ← GenerateUniqueID()\\n8: M ← M ∪ {(id, f ,\"ADD\")} ▷ Add new fact with unique identifier\\n9: else if operation = UPDATEthen\\n10: mi ← FindRelatedMemory(f , M)\\n11: if InformationContent(f ) > InformationContent(mi)then\\n12: M ← (M \\\\ {mi})∪ {(idi, f ,\"UPDATE\")} ▷ Replace with richer information\\n13: end if\\n14: else if operation = DELETE then\\n15: mi ← FindContradictedMemory(f , M)\\n16: M ← M \\\\ {mi} ▷ Remove contradicted information\\n17: else if operation = NOOPthen\\n18: No operation performed ▷ Fact already exists or is irrelevant\\n19: end if\\n20: end for\\n21: return M\\n22: end procedure\\n23: function C l as s i f y Op e r at ion( f , M)\\n24: if ¬SemanticallySimilar(f , M)then\\n25: return ADD ▷ New information not present in memory\\n26: else ifContradicts(f , M)then\\n27: return DELETE ▷ Information conflicts with existing memory\\n28: else ifAugments(f , M)then\\n29: return UPDATE ▷ Enhances existing information in memory\\n30: else\\n31: return NOOP ▷ No change required\\n32: end if\\n33: end function\\nC. Selected Baselines\\nLoCoMo The LoCoMo framework implements a sophisticated memory pipeline that enables LLM agents to\\nmaintain coherent, long-term conversations. At its core, the system divides memory into short-term and\\nlong-term components. After each conversation session, agents generate summaries (stored as short-term\\nmemory) that distill key information from that interaction. Simultaneously, individual conversation turns are\\n21'),\n",
       " Document(id='id11', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:11:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:11:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './Mem0.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11'}, page_content='Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\\nTable 2:Performance comparison of various baselines with proposed methods. Latency measurements show p50\\n(median) and p95 (95th percentile) values in seconds for both search time (time taken to fetch memories/chunks) and\\ntotal time (time to generate the complete response). Overall LLM-as-a-Judge score (J) represents the quality metric of\\nthe generated responses on the entireLOCOMO dataset.\\nMethod\\nLatency (seconds) Overall\\nJSearch Total\\nK chunk size/\\nmemory tokens p50 p95 p50 p95\\nRAG\\n1\\n128 0.281 0.823 0.774 1.825 47.77 ± 0.23%\\n256 0.251 0.710 0.745 1.628 50.15 ± 0.16%\\n512 0.240 0.639 0.772 1.710 46.05 ± 0.14%\\n1024 0.240 0.723 0.821 1.957 40.74 ± 0.17%\\n2048 0.255 0.752 0.996 2.182 37.93 ± 0.12%\\n4096 0.254 0.719 1.093 2.711 36.84 ± 0.17%\\n8192 0.279 0.838 1.396 4.416 44.53 ± 0.13%\\n2\\n128 0.267 0.624 0.766 1.829 59.56 ± 0.19%\\n256 0.255 0.699 0.802 1.907 60.97 ± 0.20%\\n512 0.247 0.746 0.829 1.729 58.19 ± 0.18%\\n1024 0.238 0.702 0.860 1.850 50.68 ± 0.13%\\n2048 0.261 0.829 1.101 2.791 48.57 ± 0.22%\\n4096 0.266 0.944 1.451 4.822 51.79 ± 0.15%\\n8192 0.288 1.124 2.312 9.942 60.53 ± 0.16%\\nFull-context 26031 - - 9.870 17.117 72.90 ± 0.19%\\nA-Mem 2520 0.668 1.485 1.410 4.374 48.38 ± 0.15%\\nLangMem 127 17.99 59.82 18.53 60.40 58.10 ± 0.21%\\nZep 3911 0.513 0.778 1.292 2.926 65.99 ± 0.16%\\nOpenAI 4437 - - 0.466 0.889 52.90 ± 0.14%\\nMem0 1764 0.148 0.200 0.708 1.440 66.88 ± 0.15%\\nMem0g 3616 0.476 0.657 1.091 2.590 68.44 ± 0.17%\\nexpected relational advantages ofMem0g do not translate into better outcomes here, suggesting potential\\noverhead or redundancy when navigating more intricate graph structures in multi-step reasoning scenarios.\\nIn temporal reasoning, Mem0g substantially outperforms other methods, validating that structured\\nrelational graphs excel in capturing chronological relationships and event sequences. The presence of explicit\\nrelational context significantly enhancesMem0g’s temporal coherence, outperformingMem0’s dense memory\\nstorage and highlighting the importance of precise relational representations when tracking temporally\\nsensitive information. Open-domain performance further reinforces the value of relational modeling.Mem0g,\\nbenefiting from the relational clarity of graph-based memory, closely competes with the top-performing\\nbaseline (Zep). This competitive result underscoresMem0g’s robustness in integrating external knowledge\\nthrough relational clarity, suggesting an optimal synergy between structured memory and open-domain\\ninformation synthesis.\\n11'),\n",
       " Document(id='id8', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:11:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:11:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './Mem0.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8'}, page_content='Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\\nrobustframeworkforevaluatingtheeffectivenessofdifferentmemoryarchitecturesacrossvariousdimensions,\\nincluding factual accuracy, computational efficiency, and scalability to extended conversations. Where\\napplicable, unless otherwise specified, we set the temperature to 0 to ensure the runs are as reproducible as\\npossible.\\nEstablished LOCOMO Benchmarks We first establish a comparative foundation by evaluating previously\\nbenchmarked methods on theLOCOMO dataset. These include five established approaches: LoCoMo (Maha-\\nrana et al., 2024), ReadAgent (Lee et al., 2024), MemoryBank (Zhong et al., 2024), MemGPT (Packer et al.,\\n2023), and A-Mem (Xu et al., 2025). These established benchmarks not only provide direct comparison\\npoints with published results but also represent the evolution of conversational memory architectures across\\ndifferent algorithmic paradigms. For our evaluation, we select the metrics wheregpt-4o-mini was used\\nfor the evaluation. More details about these benchmarks are mentioned in Appendix C.\\nOpen-Source Memory Solutions Our second category consists of promising open-source memory architec-\\ntures such as LangMem2 (Hot Path) that have demonstrated effectiveness in related conversational tasks but\\nhave not yet been evaluated on theLOCOMO dataset. By adapting these systems to our evaluation framework,\\nwe broaden the comparative landscape and identify potential alternative approaches that may offer competi-\\ntive performance. We initialized the LLM withgpt-4o-mini and usedtext-embedding-small-3 as the\\nembedding model.\\nRetrieval-Augmented Generation (RAG) As a baseline, we treat the entire conversation history as a\\ndocument collection and apply a standard RAG pipeline. We first segment each conversation into fixed-length\\nchunks (128, 256, 512, 1024, 2048, 4096, and 8192 tokens), where 8192 is the maximum chunk size\\nsupported by our embedding model. All chunks are embedded using OpenAI’stext-embedding-small-3\\nto ensure consistent vector quality across configurations. At query time, we retrieve the topk chunks by\\nsemantic similarity and concatenate them as context for answer generation. Throughout our experiments we\\nset k∈{1,2}: with k=1 only the single most relevant chunk is used, and withk=2 the two most relevant\\nchunks (up to 16384 tokens) are concatenated. We avoidk > 2 since the average conversation length (26000\\ntokens) would be fully covered, negating the benefits of selective retrieval. By varying chunk size andk, we\\nsystematically evaluate RAG performance on long-term conversational memory tasks.\\nFull-Context Processing We adopt a straightforward approach by passing the entire conversation history\\nwithin the context window of the LLM. This method leverages the model’s inherent ability to process\\nsequentialinformationwithoutadditionalarchitecturalcomponents. Whileconceptuallysimple,thisapproach\\nfaces practical limitations as conversation length increases, eventually increasing token cost and latency.\\nNevertheless, it establishes an important reference point for understanding the value of more sophisticated\\nmemory mechanisms compared to direct processing of available context.\\nProprietaryModels WeevaluateOpenAI’smemory3 featureavailableintheirChatGPTinterface,specifically\\nusing gpt-4o-mini for consistency. We ingest entireLOCOMO conversations with a prompt (see Appendix A)\\nintosinglechatsessions,promptingmemorygenerationwithtimestamps,participantnames,andconversation\\n2https://langchain-ai.github.io/langmem/\\n3https://openai.com/index/memory-and-new-controls-for-chatgpt/\\n8')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_context(\"LOCOMO\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c8cdbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_document_list(docs):\n",
    "    formatted = \"\"\n",
    "    for doc in docs:\n",
    "        formatted += doc.page_content\n",
    "    # print(formatted)\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "90c576d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\\nTable 1:Performance comparison of memory-enabled systems across different question types in theLOCOMO dataset.\\nEvaluation metrics include F1 score (F1), BLEU-1 (B1), and LLM-as-a-Judge score (J), with higher values indicating\\nbetter performance.A-Mem∗ represents results from our re-run of A-Mem to generate LLM-as-a-Judge scores by setting\\ntemperature as 0.Mem0g indicates our proposed architecture enhanced with graph memory.Bold denotes the best\\nperformance for each metric across all methods. (↑) represents higher score is better.\\nMethod Single Hop Multi-Hop Open Domain Temporal\\nF1 ↑ B1 ↑ J↑ F1 ↑ B1 ↑ J↑ F1 ↑ B1 ↑ J↑ F1 ↑ B1 ↑ J↑\\nLoCoMo 25.02 19.75 – 12.04 11.16 – 40.36 29.05 – 18.41 14.77 –\\nReadAgent 9.15 6.48 – 5.31 5.12 – 9.67 7.66 – 12.60 8.87 –\\nMemoryBank5.00 4.77 – 5.56 5.94 – 6.61 5.16 – 9.68 6.99 –\\nMemGPT 26.65 17.72 – 9.15 7.44 – 41.04 34.34 – 25.52 19.44 –\\nA-Mem 27.02 20.09 – 12.14 12.00 – 44.65 37.06 – 45.85 36.67 –\\nA-Mem* 20.76 14.90 39.79±0.38 9.22 8.81 18.85±0.31 33.34 27.58 54.05±0.22 35.40 31.08 49.91±0.31\\nLangMem 35.51 26.86 62.23±0.75 26.0422.3247.92±0.47 40.91 33.63 71.12±0.20 30.75 25.84 23.43±0.39\\nZep 35.74 23.30 61.70±0.32 19.37 14.82 41.35±0.48 49.5638.9276.60±0.13 42.00 34.53 49.31±0.50\\nOpenAI 34.30 23.72 63.79±0.46 20.09 15.42 42.92±0.63 39.31 31.16 62.29±0.12 14.04 11.25 21.71±0.20\\nMem0 38.72 27.13 67.13±0.6528.6421.5851.15±0.31 47.65 38.72 72.93±0.11 48.9340.5155.51±0.34\\nMem0g 38.09 26.03 65.71±0.45 24.32 18.82 47.19±0.67 49.2740.3075.71±0.21 51.5540.2858.13±0.44\\ntext. These generated memories are then used as complete context for answering questions about each\\nconversation, intentionally granting the OpenAI approach privileged access to all memories rather than\\nonly question-relevant ones. This methodology accommodates the lack of external API access for selective\\nmemory retrieval in OpenAI’s system for benchmarking.\\nMemoryProviders WeincorporateZep(Rasmussenetal.,2025),amemorymanagementplatformdesigned\\nfor AI agents. Using their platform version, we conduct systematic evaluations across theLOCOMO dataset,\\nmaintaining temporal fidelity by preserving timestamp information alongside conversational content. This\\ntemporalanchoringensuresthattime-sensitivequeriescanbeaddressedthroughappropriatelycontextualized\\nmemory retrieval, particularly important for evaluating questions that require chronological awareness.\\nThis baseline represents an important commercial implementation of memory management specifically\\nengineered for AI agents.\\n4. Evaluation Results, Analysis and Discussion.\\n4.1. Performance Comparison Across Memory-Enabled Systems\\nTable1reportsF 1, B1 andJscoresforourtwoarchitectures— Mem0and Mem0g —againstasuiteofcompetitive\\nbaselines, asmentionedinSection3, onsingle -hop, multi-hop, open-domain, andtemporalquestions. Overall,\\nboth of our models set new state-of-the-art marks in all the three evaluation metrics for most question types.\\nSingle-Hop Question Performance Single-hop queries involve locating a single factual span contained\\nwithin one dialogue turn. Leveraging its dense memories in natural language text,Mem0 secures the strongest\\nresults:F1=38.72, B1=27.13, and J=67.13. Augmenting the natural language memories with graph memory\\n(Mem0g) yields marginal performance drop compared toMem0, indicating that relational structure provides\\nlimited utility when the retrieval target occupies a single turn. Among the existing baselines, the full-context\\nOpenAIrun attains the next-best J score, reflecting the benefits of retaining the entire conversation in context,\\nwhile LangMem and Zep both score around 8% relatively less against our models on J score. PreviousLOCOMO\\n9Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\\nB. Algorithm\\nAlgorithm 1Memory Management System: Update Operations\\n1: Input: Set of retrieved memoriesF, Existing memory storeM = {m1, m2, . . . , mn}\\n2: Output: Updated memory storeM′\\n3: procedure Up dat e M e mory(F, M)\\n4: for each fact f ∈ F do\\n5: operation ← C l as s i f y Op e r at ion(f , M) ▷ Execute appropriate operation based on\\nclassification\\n6: if operation = ADDthen\\n7: id ← GenerateUniqueID()\\n8: M ← M ∪ {(id, f ,\"ADD\")} ▷ Add new fact with unique identifier\\n9: else if operation = UPDATEthen\\n10: mi ← FindRelatedMemory(f , M)\\n11: if InformationContent(f ) > InformationContent(mi)then\\n12: M ← (M \\\\ {mi})∪ {(idi, f ,\"UPDATE\")} ▷ Replace with richer information\\n13: end if\\n14: else if operation = DELETE then\\n15: mi ← FindContradictedMemory(f , M)\\n16: M ← M \\\\ {mi} ▷ Remove contradicted information\\n17: else if operation = NOOPthen\\n18: No operation performed ▷ Fact already exists or is irrelevant\\n19: end if\\n20: end for\\n21: return M\\n22: end procedure\\n23: function C l as s i f y Op e r at ion( f , M)\\n24: if ¬SemanticallySimilar(f , M)then\\n25: return ADD ▷ New information not present in memory\\n26: else ifContradicts(f , M)then\\n27: return DELETE ▷ Information conflicts with existing memory\\n28: else ifAugments(f , M)then\\n29: return UPDATE ▷ Enhances existing information in memory\\n30: else\\n31: return NOOP ▷ No change required\\n32: end if\\n33: end function\\nC. Selected Baselines\\nLoCoMo The LoCoMo framework implements a sophisticated memory pipeline that enables LLM agents to\\nmaintain coherent, long-term conversations. At its core, the system divides memory into short-term and\\nlong-term components. After each conversation session, agents generate summaries (stored as short-term\\nmemory) that distill key information from that interaction. Simultaneously, individual conversation turns are\\n21Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\\nTable 2:Performance comparison of various baselines with proposed methods. Latency measurements show p50\\n(median) and p95 (95th percentile) values in seconds for both search time (time taken to fetch memories/chunks) and\\ntotal time (time to generate the complete response). Overall LLM-as-a-Judge score (J) represents the quality metric of\\nthe generated responses on the entireLOCOMO dataset.\\nMethod\\nLatency (seconds) Overall\\nJSearch Total\\nK chunk size/\\nmemory tokens p50 p95 p50 p95\\nRAG\\n1\\n128 0.281 0.823 0.774 1.825 47.77 ± 0.23%\\n256 0.251 0.710 0.745 1.628 50.15 ± 0.16%\\n512 0.240 0.639 0.772 1.710 46.05 ± 0.14%\\n1024 0.240 0.723 0.821 1.957 40.74 ± 0.17%\\n2048 0.255 0.752 0.996 2.182 37.93 ± 0.12%\\n4096 0.254 0.719 1.093 2.711 36.84 ± 0.17%\\n8192 0.279 0.838 1.396 4.416 44.53 ± 0.13%\\n2\\n128 0.267 0.624 0.766 1.829 59.56 ± 0.19%\\n256 0.255 0.699 0.802 1.907 60.97 ± 0.20%\\n512 0.247 0.746 0.829 1.729 58.19 ± 0.18%\\n1024 0.238 0.702 0.860 1.850 50.68 ± 0.13%\\n2048 0.261 0.829 1.101 2.791 48.57 ± 0.22%\\n4096 0.266 0.944 1.451 4.822 51.79 ± 0.15%\\n8192 0.288 1.124 2.312 9.942 60.53 ± 0.16%\\nFull-context 26031 - - 9.870 17.117 72.90 ± 0.19%\\nA-Mem 2520 0.668 1.485 1.410 4.374 48.38 ± 0.15%\\nLangMem 127 17.99 59.82 18.53 60.40 58.10 ± 0.21%\\nZep 3911 0.513 0.778 1.292 2.926 65.99 ± 0.16%\\nOpenAI 4437 - - 0.466 0.889 52.90 ± 0.14%\\nMem0 1764 0.148 0.200 0.708 1.440 66.88 ± 0.15%\\nMem0g 3616 0.476 0.657 1.091 2.590 68.44 ± 0.17%\\nexpected relational advantages ofMem0g do not translate into better outcomes here, suggesting potential\\noverhead or redundancy when navigating more intricate graph structures in multi-step reasoning scenarios.\\nIn temporal reasoning, Mem0g substantially outperforms other methods, validating that structured\\nrelational graphs excel in capturing chronological relationships and event sequences. The presence of explicit\\nrelational context significantly enhancesMem0g’s temporal coherence, outperformingMem0’s dense memory\\nstorage and highlighting the importance of precise relational representations when tracking temporally\\nsensitive information. Open-domain performance further reinforces the value of relational modeling.Mem0g,\\nbenefiting from the relational clarity of graph-based memory, closely competes with the top-performing\\nbaseline (Zep). This competitive result underscoresMem0g’s robustness in integrating external knowledge\\nthrough relational clarity, suggesting an optimal synergy between structured memory and open-domain\\ninformation synthesis.\\n11Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory\\nrobustframeworkforevaluatingtheeffectivenessofdifferentmemoryarchitecturesacrossvariousdimensions,\\nincluding factual accuracy, computational efficiency, and scalability to extended conversations. Where\\napplicable, unless otherwise specified, we set the temperature to 0 to ensure the runs are as reproducible as\\npossible.\\nEstablished LOCOMO Benchmarks We first establish a comparative foundation by evaluating previously\\nbenchmarked methods on theLOCOMO dataset. These include five established approaches: LoCoMo (Maha-\\nrana et al., 2024), ReadAgent (Lee et al., 2024), MemoryBank (Zhong et al., 2024), MemGPT (Packer et al.,\\n2023), and A-Mem (Xu et al., 2025). These established benchmarks not only provide direct comparison\\npoints with published results but also represent the evolution of conversational memory architectures across\\ndifferent algorithmic paradigms. For our evaluation, we select the metrics wheregpt-4o-mini was used\\nfor the evaluation. More details about these benchmarks are mentioned in Appendix C.\\nOpen-Source Memory Solutions Our second category consists of promising open-source memory architec-\\ntures such as LangMem2 (Hot Path) that have demonstrated effectiveness in related conversational tasks but\\nhave not yet been evaluated on theLOCOMO dataset. By adapting these systems to our evaluation framework,\\nwe broaden the comparative landscape and identify potential alternative approaches that may offer competi-\\ntive performance. We initialized the LLM withgpt-4o-mini and usedtext-embedding-small-3 as the\\nembedding model.\\nRetrieval-Augmented Generation (RAG) As a baseline, we treat the entire conversation history as a\\ndocument collection and apply a standard RAG pipeline. We first segment each conversation into fixed-length\\nchunks (128, 256, 512, 1024, 2048, 4096, and 8192 tokens), where 8192 is the maximum chunk size\\nsupported by our embedding model. All chunks are embedded using OpenAI’stext-embedding-small-3\\nto ensure consistent vector quality across configurations. At query time, we retrieve the topk chunks by\\nsemantic similarity and concatenate them as context for answer generation. Throughout our experiments we\\nset k∈{1,2}: with k=1 only the single most relevant chunk is used, and withk=2 the two most relevant\\nchunks (up to 16384 tokens) are concatenated. We avoidk > 2 since the average conversation length (26000\\ntokens) would be fully covered, negating the benefits of selective retrieval. By varying chunk size andk, we\\nsystematically evaluate RAG performance on long-term conversational memory tasks.\\nFull-Context Processing We adopt a straightforward approach by passing the entire conversation history\\nwithin the context window of the LLM. This method leverages the model’s inherent ability to process\\nsequentialinformationwithoutadditionalarchitecturalcomponents. Whileconceptuallysimple,thisapproach\\nfaces practical limitations as conversation length increases, eventually increasing token cost and latency.\\nNevertheless, it establishes an important reference point for understanding the value of more sophisticated\\nmemory mechanisms compared to direct processing of available context.\\nProprietaryModels WeevaluateOpenAI’smemory3 featureavailableintheirChatGPTinterface,specifically\\nusing gpt-4o-mini for consistency. We ingest entireLOCOMO conversations with a prompt (see Appendix A)\\nintosinglechatsessions,promptingmemorygenerationwithtimestamps,participantnames,andconversation\\n2https://langchain-ai.github.io/langmem/\\n3https://openai.com/index/memory-and-new-controls-for-chatgpt/\\n8'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_document_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "178a16c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You are Mohit and you are AI expert,\n",
    "        you have access to all the docs and research paper.\n",
    "        You will get a prompt and some context, go through the context\n",
    "        and explain that concept to the user.\n",
    "\n",
    "        Your tone should be friendly. Do not use emojis.\n",
    "        RESPOND IN PLAIN TEXT, do not use MarkDown syntax.\n",
    "        \"\"\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1b8319c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def print_llm_response(prompt):\n",
    "    similar_docs = get_context(f\"PROMPT: {prompt}. Explain!\")\n",
    "    formatted_context = format_document_list(similar_docs[:3])\n",
    "\n",
    "    messages.append((\"human\", f\"\"\"PROMPT: {prompt} \\nCONTEXT: {formatted_context}\"\"\"))\n",
    "\n",
    "    llm = ChatGroq(\n",
    "        model=\"qwen/qwen3-32b\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        reasoning_format=\"parsed\",\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        streaming=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    for chunk in llm.stream(messages):\n",
    "        response += chunk.content\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "    messages.append((\"assistant\", response))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "47a34377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text(filename, content):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6c053e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper **Mem0** focuses on building AI agents with **scalable long-term memory** for production use. It introduces systems and prompts (like memory retrieval templates) to:  \n",
      "1. **Store and organize** user conversation data with timestamps and relationships.  \n",
      "2. **Retrieve precise information** from past interactions to generate accurate, context-aware responses.  \n",
      "3. **Handle time-sensitive queries** by converting relative time references (e.g., \"yesterday\") to specific dates.  \n",
      "4. **Evaluate response accuracy** using a \"LLM-as-a-Judge\" framework to ensure answers align with historical data.  \n",
      "\n",
      "The goal is to create reliable, production-ready AI systems that retain and utilize user-specific memory effectively."
     ]
    }
   ],
   "source": [
    "response = print_llm_response(\"What is this research paper about?\")\n",
    "save_text(\"AI_Response.md\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maventic_ai_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
